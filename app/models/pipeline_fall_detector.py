# app/models/pipeline_fall_detector.py
import sys                          # Th∆∞ vi·ªán h·ªá th·ªëng ƒë·ªÉ thao t√°c v·ªõi Python interpreter
import os                           # Th∆∞ vi·ªán h·ªá ƒëi·ªÅu h√†nh ƒë·ªÉ l√†m vi·ªác v·ªõi files/folders
from pathlib import Path            # Th∆∞ vi·ªán hi·ªán ƒë·∫°i ƒë·ªÉ x·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n files
import numpy as np                  # Th∆∞ vi·ªán t√≠nh to√°n s·ªë h·ªçc, x·ª≠ l√Ω arrays
from PIL import Image              # Python Imaging Library ƒë·ªÉ x·ª≠ l√Ω ·∫£nh
import logging                     # Th∆∞ vi·ªán ghi log, debug v√† monitor
from typing import Dict, Any, Optional, List  # Type hints ƒë·ªÉ l√†m r√µ ki·ªÉu d·ªØ li·ªáu
import time                        # Th∆∞ vi·ªán x·ª≠ l√Ω th·ªùi gian
import json                        # Th∆∞ vi·ªán x·ª≠ l√Ω d·ªØ li·ªáu JSON
from collections import deque      # Circular buffer v·ªõi k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh

# Add src to path
# Th√™m th∆∞ m·ª•c src v√†o Python path ƒë·ªÉ c√≥ th·ªÉ import pipeline
sys.path.append(str(Path(__file__).parent.parent.parent / "src"))

try:
    from src.pipeline.fall_detect import FallDetector as PipelineFallDetector
except ImportError as e:
     # N·∫øu import th·∫•t b·∫°i, in th√¥ng b√°o l·ªói r√µ r√†ng
    print(f" Cannot import pipeline fall detector: {e}")
    print("Please ensure pipeline files are in src/pipeline/")
    raise # N√©m l·∫°i exception ƒë·ªÉ d·ª´ng ch∆∞∆°ng tr√¨nh

class ProductionPipelineFallDetector:
    """
    Class wrapper k·∫øt h·ª£p pipeline v·ªõi enhanced features
    """
    def __init__(self, 
                 model_path: str,   # ƒê∆∞·ªùng d·∫´n ƒë·∫øn file model .tflite
                 model_name: str = "mobilenet",  # Lo·∫°i model: "mobilenet" ho·∫∑c "movenet"  
                 confidence_threshold: float = 0.6, # Ng∆∞·ª°ng tin c·∫≠y
                 config: Dict = None): # C·∫•u h√¨nh b·ªï sung (optional)
        
        # Kh·ªüi t·∫°o logger ƒë·ªÉ ghi log cho class n√†y
        self.logger = logging.getLogger(__name__)
        # N·∫øu kh√¥ng c√≥ config th√¨ d√πng dict r·ªóng
        # config or {} = n·∫øu config l√† None th√¨ d√πng {}
        self.config = config or {}
        
        # Model configuration
        # Chu·∫©n b·ªã c·∫•u h√¨nh model cho pipeline
        # Pipeline y√™u c·∫ßu dict c√≥ 2 keys: 'tflite' v√† 'edgetpu'
        model_config = {
            'tflite': model_path,  # Model TensorFlow Lite th√¥ng th∆∞·ªùng
            'edgetpu': self.config.get('edgetpu_model_path', None)# Model EdgeTPU (hardware acceleration)
        }
        
        # Create labels file
        # Pipeline y√™u c·∫ßu file labels (d√π kh√¥ng d√πng cho pose detection)
        # T·∫°o file labels gi·∫£ ƒë·ªÉ th·ªèa m√£n y√™u c·∫ßu c·ªßa pipeline
        labels_path = self._create_labels_file()
        
        try:
            # Initialize pipeline fall detector
            # Kh·ªüi t·∫°o pipeline fall detector
            self.fall_detector = PipelineFallDetector(
                model=model_config,                    # C·∫•u h√¨nh model
                labels=labels_path,                    # File labels (gi·∫£)
                confidence_threshold=confidence_threshold,  # Ng∆∞·ª°ng tin c·∫≠y
                model_name=model_name                  # Lo·∫°i model
            )
            # Ghi log th√†nh c√¥ng
            self.logger.info(f" Pipeline Fall Detector initialized")
            self.logger.info(f"   Model: {model_name}")
            self.logger.info(f"   Confidence threshold: {confidence_threshold}")
            # Ki·ªÉm tra xem c√≥ EdgeTPU kh√¥ng
            self.logger.info(f"   EdgeTPU: {'Yes' if model_config['edgetpu'] else 'No'}")
            
        except Exception as e:
            # N·∫øu kh·ªüi t·∫°o th·∫•t b·∫°i, ghi log l·ªói v√† n√©m exception
            self.logger.error(f" Failed to initialize pipeline fall detector: {e}")
            raise
        
        ##KH·ªûI T·∫†O TRACKING SYSTEM
        # Enhanced tracking
        # Enhanced tracking v·ªõi circular buffers
        # deque = double-ended queue, efficient cho add/remove ·ªü ƒë·∫ßu/cu·ªëi
        # maxlen=20 = t·ª± ƒë·ªông x√≥a ph·∫ßn t·ª≠ c≈© nh·∫•t khi v∆∞·ª£t qu√° 20 ph·∫ßn t·ª≠
        self.detection_history = deque(maxlen=20)     # L∆∞u 20 k·∫øt qu·∫£ detection g·∫ßn nh·∫•t
        self.performance_history = deque(maxlen=100)  # L∆∞u 100 th·ªùi gian x·ª≠ l√Ω g·∫ßn nh·∫•
        
        # Statistics
        # Dictionary l∆∞u tr·ªØ th·ªëng k√™ t·ªïng quan
        self.stats = {
            'total_processed': 0,              # T·ªïng s·ªë frame ƒë√£ x·ª≠ l√Ω t·ª´ ƒë·∫ßu session
            'falls_detected': 0,               # T·ªïng s·ªë l·∫ßn ph√°t hi·ªán t√© ng√£
            'false_positives_filtered': 0,     # S·ªë l·∫ßn filter false positive
            'avg_processing_time': 0.0,        # Th·ªùi gian x·ª≠ l√Ω trung b√¨nh (gi√¢y)
            'avg_confidence': 0.0,             # Confidence trung b√¨nh c·ªßa t·∫•t c·∫£ detections
            'last_detection_time': None,       # Timestamp l·∫ßn cu·ªëi ph√°t hi·ªán t√© ng√£
            'session_start_time': time.time()  # Th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu session (Unix timestamp)
        }
        # Configuration
        # C·∫•u h√¨nh cho temporal analysis (ph√¢n t√≠ch theo th·ªùi gian)
        # .get() = l·∫•y gi√° tr·ªã t·ª´ dict, n·∫øu kh√¥ng c√≥ th√¨ d√πng default value
        self.temporal_window = self.config.get('temporal_window', 3) # Xem 3 detections g·∫ßn nh·∫•t
        self.confidence_boost_factor = self.config.get('confidence_boost_factor', 1.2)# TƒÉng confidence 20% khi c√≥ temporal evidence
        self.false_positive_threshold = self.config.get('false_positive_threshold', 2) #C·∫ßn √≠t nh·∫•t 2 falls trong window ƒë·ªÉ confirm
        
    def _create_labels_file(self) -> str:
        """
        T·∫°o file labels gi·∫£ v√¨ pipeline y√™u c·∫ßu labels file
        D√π kh√¥ng th·ª±c s·ª± d√πng cho pose detection
        """
        labels_path = "temp_labels.txt"  # T√™n file labels t·∫°m th·ªùi
        
        # Ki·ªÉm tra n·∫øu file ch∆∞a t·ªìn t·∫°i th√¨ t·∫°o m·ªõi
        if not os.path.exists(labels_path):
            # M·ªü file ·ªü mode write ('w') v√† t·ª± ƒë·ªông ƒë√≥ng khi xong
            with open(labels_path, 'w') as f:
                f.write("person\n")
        
        return labels_path
    
    ##H√ÄM X·ª¨ L√ù CH√çNH
    def process_image(self, image: Image.Image) -> Optional[Dict[str, Any]]:
        """
        H√†m x·ª≠ l√Ω ch√≠nh - nh·∫≠n ·∫£nh PIL v√† tr·∫£ v·ªÅ k·∫øt qu·∫£ detection
        
        Args:
            image: PIL Image object
            
        Returns:
            Dict ch·ª©a k·∫øt qu·∫£ detection ho·∫∑c None n·∫øu th·∫•t b·∫°i
        """
        try:
            # Ghi l·∫°i th·ªùi ƒëi·ªÉm b·∫Øt ƒë·∫ßu x·ª≠ l√Ω
            start_time = time.time()
            
            # Core pipeline detection
            # G·ªçi pipeline fall detector (code t·ª´ GitHub)
            # fall_detect() tr·∫£ v·ªÅ (inference_result, thumbnail)
            inference_result, thumbnail = self.fall_detector.fall_detect(image=image)
            
            # T√≠nh th·ªùi gian x·ª≠ l√Ω
            processing_time = time.time() - start_time
            
            # Convert and enhance result
            # Convert k·∫øt qu·∫£ pipeline sang format chu·∫©n c·ªßa h·ªá th·ªëng
            result = self._process_detection_result(
                inference_result,    # K·∫øt qu·∫£ raw t·ª´ pipeline
                thumbnail,          # ·∫¢nh thumbnail ƒë√£ resize
                processing_time,    # Th·ªùi gian x·ª≠ l√Ω
                image.size         # K√≠ch th∆∞·ªõc ·∫£nh g·ªëc (width, height)
            )
            
            # Update tracking and stats
            # C·∫≠p nh·∫≠t tracking v√† statistics
            self._update_tracking(result, processing_time)
            
            # Enhanced decision making
            # √Åp d·ª•ng enhanced decision making v·ªõi temporal analysis
            result = self._enhance_decision(result)
            
            return result
            
        except Exception as e:
            self.logger.error(f" Image processing failed: {e}")
            return None
    
    ##X·ª¨ L√ù K·∫æT QU·∫¢ PIPELINE
    def _process_detection_result(self, inference_result, thumbnail, processing_time, image_size):
        """Convert k·∫øt qu·∫£ raw t·ª´ pipeline sang format chu·∫©n"""
        
        # T·∫°o structure k·∫øt qu·∫£ c∆° b·∫£n
        base_result = {
            "fall_detected": False,           # C√≥ ph√°t hi·ªán t√© ng√£ kh√¥ng
            "confidence": 0.0,               # ƒê·ªô tin c·∫≠y (0.0 - 1.0)
            "processing_time": float(processing_time),  # Th·ªùi gian x·ª≠ l√Ω (gi√¢y)
            "timestamp": time.time(),        # Timestamp hi·ªán t·∫°i (Unix time)
            "image_size": image_size,        # (width, height) c·ªßa ·∫£nh g·ªëc
            "pose_data": None,               # D·ªØ li·ªáu pose s·∫Ω ƒë∆∞·ª£c ƒëi·ªÅn sau
            "thumbnail": thumbnail           # ·∫¢nh thumbnail t·ª´ pipeline
        }
        
        # Ki·ªÉm tra n·∫øu pipeline kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ g√¨
        if not inference_result:
            # C·∫≠p nh·∫≠t th√¥ng tin cho tr∆∞·ªùng h·ª£p kh√¥ng c√≥ detection
            base_result.update({
                "reason": "no_fall_detected",       # L√Ω do kh√¥ng detect
                "pipeline_result": None             # Kh√¥ng c√≥ k·∫øt qu·∫£ t·ª´ pipeline
            })
            return base_result
        
        # Get primary detection result
        # Parse k·∫øt qu·∫£ t·ª´ pipeline
        # Pipeline tr·∫£ v·ªÅ list, l·∫•y k·∫øt qu·∫£ ƒë·∫ßu ti√™n (confidence cao nh·∫•t)
        primary_result = inference_result[0]
        label, confidence, leaning_angle, keypoint_corr = primary_result
        
        # Enhanced fall detection
        # Logic quy·∫øt ƒë·ªãnh c√≥ ph·∫£i t√© ng√£ kh√¥ng
        # Ph·∫£i c√≥ label "FALL" V√Ä confidence > threshold
        fall_detected = (label == "FALL" and confidence > self.fall_detector.confidence_threshold)
        
        # Extract detailed pose data
        # Extract th√¥ng tin pose chi ti·∫øt t·ª´ keypoint correlation
        pose_data = self._extract_detailed_pose_data(keypoint_corr, leaning_angle)
        
        # Enhanced result
        # T·∫°o k·∫øt qu·∫£ enhanced v·ªõi th√¥ng tin chi ti·∫øt
        enhanced_result = base_result.copy()
        enhanced_result.update({
            "fall_detected":  bool(fall_detected),
            "confidence": float(confidence),        # Ensure Python float (not numpy)
            "leaning_angle": float(leaning_angle),  # G√≥c nghi√™ng c∆° th·ªÉ (ƒë·ªô)
            "label": str(label),                         # Label t·ª´ pipeline ("FALL" ho·∫∑c kh√°c)
            "pose_data": pose_data,                 # Structured pose data
            "keypoint_correlation": keypoint_corr,  # Raw keypoint data t·ª´ pipeline
            "pipeline_result": {                    # Metadata t·ª´ pipeline
                "raw_inference": inference_result,     # To√†n b·ªô k·∫øt qu·∫£ raw
                "spinal_vector_score": float(confidence),     # Pipeline's spinal vector score
                "body_line_detected": bool(keypoint_corr)  # C√≥ detect ƒë∆∞·ª£c body line kh√¥ng
            }
        })
        
        return enhanced_result
    
    ##EXTRACT POSE DATA CHI TI·∫æT
    def _extract_detailed_pose_data(self, keypoint_corr, leaning_angle):
        """Extract d·ªØ li·ªáu pose structured t·ª´ keypoint correlation c·ªßa pipeline"""
        # N·∫øu kh√¥ng c√≥ keypoint correlation th√¨ return None
        if not keypoint_corr:
            return None
        
        # Kh·ªüi t·∫°o lists ƒë·ªÉ l∆∞u keypoints v√† body lines
        keypoints = []      # List c√°c keypoint ƒë√£ structured
        body_lines = []     # List c√°c ƒë∆∞·ªùng k·∫øt n·ªëi c∆° th·ªÉ
    
        
        # Mapping t·ª´ t√™n keypoint c·ªßa pipeline sang format chu·∫©n
        # Pipeline d√πng: 'left shoulder', h·ªá th·ªëng d√πng: 'left_shoulder'
        # Tuple: (t√™n_chu·∫©n, index_trong_COCO_format)
        keypoint_mapping = {
            'left shoulder': ('left_shoulder', 5),
            'right shoulder': ('right_shoulder', 6),
            'left hip': ('left_hip', 11),
            'right hip': ('right_hip', 12)
        }
        
        # Extract keypoints
        # Loop qua t·∫•t c·∫£ keypoints trong correlation
        for name, coords in keypoint_corr.items():
            # Ki·ªÉm tra n·∫øu c√≥ coordinates V√Ä t√™n keypoint n·∫±m trong mapping
            if coords and name in keypoint_mapping:
                # L·∫•y t√™n chu·∫©n v√† index t·ª´ mapping
                kp_name, kp_index = keypoint_mapping[name]
                # Unpack coordinates: (x, y)
                x, y = coords
                # T·∫°o keypoint object theo format chu·∫©n
                keypoints.append({
                'name': kp_name,              # T√™n keypoint chu·∫©n
                'index': kp_index,            # Index trong COCO format
                'x': float(x),                # T·ªça ƒë·ªô X (pixel)
                'y': float(y),                # T·ªça ƒë·ªô Y (pixel)
                'confidence': 1.0             # Pipeline kh√¥ng cung c·∫•p individual confidence
            })
        
        # Extract body lines (ƒë∆∞·ªùng k·∫øt n·ªëi shoulder-hip)
        # Ki·ªÉm tra n·∫øu c√≥ c·∫£ left shoulder v√† left hip
        if ('left shoulder' in keypoint_corr and 'left hip' in keypoint_corr):
            left_line = {
                'type': 'left_body_line',                    # Lo·∫°i ƒë∆∞·ªùng k·∫øt n·ªëi
                'start':[float(keypoint_corr['left shoulder'][0]), 
                     float(keypoint_corr['left shoulder'][1])],     # ƒêi·ªÉm b·∫Øt ƒë·∫ßu (x1, y1)
                'end': [float(keypoint_corr['left hip'][0]), 
                   float(keypoint_corr['left hip'][1])]             # ƒêi·ªÉm k·∫øt th√∫c (x2, y2)
            }
            body_lines.append(left_line)
        
        # T∆∞∆°ng t·ª± cho right body line
        if ('right shoulder' in keypoint_corr and 'right hip' in keypoint_corr):
            right_line = {
                'type': 'right_body_line', 
                'start': [float(keypoint_corr['right shoulder'][0]), 
                     float(keypoint_corr['right shoulder'][1])],
                'end': [float(keypoint_corr['right hip'][0]), 
                   float(keypoint_corr['right hip'][1])]
            }
            body_lines.append(right_line)
        # Tr·∫£ v·ªÅ structured pose data
        return {
        'keypoints': keypoints,                    # List keypoints ƒë√£ format
        'body_lines': body_lines,                  # List ƒë∆∞·ªùng k·∫øt n·ªëi c∆° th·ªÉ
        'leaning_angle': float(leaning_angle),            # G√≥c nghi√™ng t·ª´ pipeline
        'keypoint_count': len(keypoints),          # S·ªë keypoints detected
        'body_line_count': len(body_lines),        # S·ªë body lines detected  
        'timestamp': time.time()                   # Timestamp t·∫°o data
    }
    
    ##C·∫¨P NH·∫¨T TRACKING V√Ä STATISTICS
    def _update_tracking(self, result, processing_time):
        """Update detection tracking and statistics"""
        
        # Add to history
        # T·∫°o entry cho detection history
        history_entry = {
            'timestamp': result['timestamp'],               # Th·ªùi ƒëi·ªÉm detection
            'fall_detected': result['fall_detected'],       # C√≥ t√© ng√£ kh√¥ng
            'confidence': result['confidence'],             # ƒê·ªô tin c·∫≠y
            'leaning_angle': result.get('leaning_angle', 0), # G√≥c nghi√™ng (default 0 n·∫øu kh√¥ng c√≥)
            'processing_time': processing_time              # Th·ªùi gian x·ª≠ l√Ω frame n√†y
        }
        
        # Th√™m v√†o detection history
        # deque v·ªõi maxlen=20 s·∫Ω t·ª± ƒë·ªông remove ph·∫ßn t·ª≠ c≈© nh·∫•t khi add ph·∫ßn t·ª≠ th·ª© 21
        self.detection_history.append(history_entry)
        
        # Th√™m processing time v√†o performance history  
        # deque v·ªõi maxlen=100 s·∫Ω t·ª± ƒë·ªông remove th·ªùi gian c≈© nh·∫•t
        self.performance_history.append(processing_time)
        
        # C·∫≠p nh·∫≠t counter statistics
        self.stats['total_processed'] += 1  # TƒÉng s·ªë frame ƒë√£ x·ª≠ l√Ω
        
        # N·∫øu ph√°t hi·ªán t√© ng√£
        if result['fall_detected']:
            self.stats['falls_detected'] += 1              # TƒÉng counter t√© ng√£
            self.stats['last_detection_time'] = time.time() # C·∫≠p nh·∫≠t th·ªùi ƒëi·ªÉm detect cu·ªëi
        
        # C·∫≠p nh·∫≠t moving averages
        if self.stats['total_processed'] == 1:
            # Frame ƒë·∫ßu ti√™n - kh·ªüi t·∫°o averages
            self.stats['avg_processing_time'] = processing_time
            self.stats['avg_confidence'] = result['confidence']
        else:
            # Exponential Moving Average (EMA) ƒë·ªÉ smooth averages
            # EMA = Œ± √ó current_value + (1-Œ±) √ó previous_average
            # Œ± = 0.1 = weight c·ªßa gi√° tr·ªã hi·ªán t·∫°i (10%), 90% weight cho history
            alpha = 0.1
            
            # C·∫≠p nh·∫≠t average processing time
            self.stats['avg_processing_time'] = (
                alpha * processing_time +                           # 10% current
                (1 - alpha) * self.stats['avg_processing_time']     # 90% history
            )
            
            # C·∫≠p nh·∫≠t average confidence
            self.stats['avg_confidence'] = (
                alpha * result['confidence'] +                      # 10% current  
                (1 - alpha) * self.stats['avg_confidence']          # 90% history
            )
    
    def _enhance_decision(self, result):
        """
        Enhanced decision making v·ªõi temporal analysis v√† false positive filtering
        """
        
        # N·∫øu ch∆∞a c√≥ ƒë·ªß history th√¨ return nguy√™n result
        if len(self.detection_history) < 2:
            return result
        
        # L·∫•y c√°c detections g·∫ßn nh·∫•t trong temporal window
        # [-self.temporal_window:] = l·∫•y N ph·∫ßn t·ª≠ cu·ªëi c√πng
        # list() = convert deque sang list ƒë·ªÉ d·ªÖ x·ª≠ l√Ω
        recent_detections = list(self.detection_history)[-self.temporal_window:]
        
        # ƒê·∫øm s·ªë l·∫ßn ph√°t hi·ªán fall trong window
        # sum() v·ªõi generator expression ƒë·ªÉ ƒë·∫øm
        recent_falls = sum(1 for d in recent_detections if d['fall_detected'])
        
        # L·∫•y t·∫•t c·∫£ confidence scores c·ªßa c√°c fall detections
        # List comprehension v·ªõi condition
        recent_confidences = [d['confidence'] for d in recent_detections if d['fall_detected']]
        
        # Logic temporal confirmation
        # C·∫ßn √≠t nh·∫•t 2 falls HO·∫∂C √≠t nh·∫•t m·ªôt n·ª≠a window size
        # max() ƒë·ªÉ ƒë·∫£m b·∫£o √≠t nh·∫•t 2 falls
        required_falls = max(2, self.temporal_window // 2)  # temporal_window=3 ‚Üí required=2
        temporal_confirmation = recent_falls >= required_falls
        
        # Confidence boosting n·∫øu c√≥ temporal evidence
        if temporal_confirmation and recent_confidences:
            # L·∫•y confidence cao nh·∫•t trong c√°c falls g·∫ßn ƒë√¢y
            max_confidence = max(recent_confidences)
            
            # Boost confidence b·∫±ng c√°ch nh√¢n v·ªõi factor
            boosted_confidence = max_confidence * self.confidence_boost_factor  # 1.2
            
            # Gi·ªõi h·∫°n confidence t·ªëi ƒëa ·ªü 0.95 (95%)
            boosted_confidence = min(0.95, boosted_confidence)
            
            # C·∫≠p nh·∫≠t k·∫øt qu·∫£
            result['confidence'] = boosted_confidence
            result['confidence_boosted'] = True      # Flag ƒë·ªÉ bi·∫øt ƒë√£ boost
        else:
            result['confidence_boosted'] = False     # Kh√¥ng boost
        
        # False positive filtering
        if result['fall_detected'] and not temporal_confirmation:
            # N·∫øu detect fall NH∆ØNG kh√¥ng c√≥ temporal confirmation
            
            # Ki·ªÉm tra n·∫øu s·ªë falls < threshold
            if recent_falls < self.false_positive_threshold:  # threshold=2
                # Filter as false positive
                self.stats['false_positives_filtered'] += 1  # TƒÉng counter FP
                result['fall_detected'] = False              # Set th√†nh kh√¥ng c√≥ fall
                result['filtered_as_false_positive'] = True  # Flag ƒë·ªÉ bi·∫øt ƒë√£ filter
                self.logger.debug("Filtered potential false positive")  # Log debug
        
        # Th√™m metadata v·ªÅ temporal analysis v√†o result
        result.update({
            'temporal_confirmation': temporal_confirmation,     # C√≥ temporal confirmation kh√¥ng
            'recent_falls_count': recent_falls,                # S·ªë falls trong window
            'temporal_window_size': len(recent_detections),    # K√≠ch th∆∞·ªõc window th·ª±c t·∫ø
            'confidence_trend': self._calculate_confidence_trend()  # Trend c·ªßa confidence
        })
        
        return result  # Tr·∫£ v·ªÅ k·∫øt qu·∫£ ƒë√£ enhanced
    
    def _calculate_confidence_trend(self):
        """
        T√≠nh trend c·ªßa confidence qua c√°c detections g·∫ßn ƒë√¢y
        """
        
        # C·∫ßn √≠t nh·∫•t 3 detections ƒë·ªÉ t√≠nh trend
        if len(self.detection_history) < 3:
            return 0.0  # Kh√¥ng ƒë·ªß data
        
        # L·∫•y 3 detections g·∫ßn nh·∫•t
        recent = list(self.detection_history)[-3:]
        
        # Extract confidence values
        confidences = [d['confidence'] for d in recent]
        
        # T√≠nh trend ƒë∆°n gi·∫£n: confidence_latest - confidence_previous
        if len(confidences) >= 2:
            # confidences[-1] = confidence m·ªõi nh·∫•t
            # confidences[-2] = confidence tr∆∞·ªõc ƒë√≥
            trend = confidences[-1] - confidences[-2]
            return trend  # Positive = tƒÉng, Negative = gi·∫£m
        
        return 0.0  # Default n·∫øu kh√¥ng t√≠nh ƒë∆∞·ª£c
    
    def get_detailed_stats(self) -> Dict[str, Any]:
        """
        L·∫•y statistics chi ti·∫øt v·ªõi performance analysis
        """
        current_time = time.time()  # Th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
        
        # T√≠nh th·ªùi gian session ƒë√£ ch·∫°y
        session_duration = current_time - self.stats['session_start_time']
        
        # T√≠nh performance statistics t·ª´ history
        performance_stats = {}  # Dict r·ªóng l√†m default
        
        if self.performance_history:  # N·∫øu c√≥ d·ªØ li·ªáu performance
            # Convert deque sang list ƒë·ªÉ d·ªÖ x·ª≠ l√Ω
            times = list(self.performance_history)
            
            performance_stats = {
                'min_processing_time': min(times),              # Th·ªùi gian x·ª≠ l√Ω nhanh nh·∫•t
                'max_processing_time': max(times),              # Th·ªùi gian x·ª≠ l√Ω ch·∫≠m nh·∫•t
                'median_processing_time': np.median(times),     # Median (gi√° tr·ªã gi·ªØa)
                'std_processing_time': np.std(times)            # Standard deviation (ƒë·ªô bi·∫øn thi√™n)
            }
        
        # Tr·∫£ v·ªÅ dictionary comprehensive
        return {
            **self.stats,                                       # Unpack t·∫•t c·∫£ basic stats
            'session_duration': session_duration,              # T·ªïng th·ªùi gian session
            
            # T√≠nh detection rate = falls/total_frames
            'detection_rate': self.stats['falls_detected'] / max(1, self.stats['total_processed']),
            
            # T√≠nh processing FPS = 1/avg_processing_time  
            'processing_fps': 1 / max(0.001, self.stats['avg_processing_time']),
            
            # T√≠nh false positive rate = FP_filtered/total_frames
            'false_positive_rate': self.stats['false_positives_filtered'] / max(1, self.stats['total_processed']),
            
            'performance_stats': performance_stats,             # Chi ti·∫øt performance
            'history_size': len(self.detection_history)        # K√≠ch th∆∞·ªõc history hi·ªán t·∫°i
        }
    
    def export_detection_history(self, filepath: str = None):
        """
        Export to√†n b·ªô detection history ra file JSON ƒë·ªÉ ph√¢n t√≠ch
        """
        
        # N·∫øu kh√¥ng ch·ªâ ƒë·ªãnh filepath th√¨ t·ª± t·∫°o
        if not filepath:
            timestamp = int(time.time())  # Unix timestamp hi·ªán t·∫°i
            filepath = f"detection_history_{timestamp}.json"  # f-string formatting
        
        # T·∫°o comprehensive data ƒë·ªÉ export
        history_data = {
            'export_timestamp': time.time(),                    # Th·ªùi ƒëi·ªÉm export
            'stats': self.get_detailed_stats(),                 # T·∫•t c·∫£ statistics
            'detection_history': list(self.detection_history),  # Convert deque‚Üílist
            'config': self.config                               # Configuration ƒë√£ d√πng
        }
        
        # Ghi ra file JSON
        with open(filepath, 'w') as f:
            json.dump(
                history_data,           # Data c·∫ßn ghi
                f,                      # File handle
                indent=2,               # Pretty print v·ªõi indent 2 spaces
                default=str             # Convert non-serializable objects sang string
            )
        
        # Log th√¥ng b√°o ƒë√£ export
        self.logger.info(f" Detection history exported to: {filepath}")
        return filepath  # Tr·∫£ v·ªÅ ƒë∆∞·ªùng d·∫´n file ƒë√£ t·∫°o
    
    def reset_stats(self):
        """
        Reset t·∫•t c·∫£ statistics v√† history ƒë·ªÉ b·∫Øt ƒë·∫ßu session m·ªõi
        """
        
        # Clear t·∫•t c·∫£ history
        self.detection_history.clear()      # X√≥a detection history
        self.performance_history.clear()    # X√≥a performance history
        
        # Reset stats v·ªÅ gi√° tr·ªã ban ƒë·∫ßu
        self.stats = {
            'total_processed': 0,                   # Reset v·ªÅ 0
            'falls_detected': 0,                    # Reset v·ªÅ 0  
            'false_positives_filtered': 0,          # Reset v·ªÅ 0
            'avg_processing_time': 0.0,             # Reset v·ªÅ 0.0
            'avg_confidence': 0.0,                  # Reset v·ªÅ 0.0
            'last_detection_time': None,            # Reset v·ªÅ None
            'session_start_time': time.time()       # Reset v·ªÅ th·ªùi ƒëi·ªÉm hi·ªán t·∫°i
        }
        
        # Log th√¥ng b√°o ƒë√£ reset
        self.logger.info("üìä Statistics and history reset")

# Alias for easier import
EnhancedPipelineFallDetector = ProductionPipelineFallDetector

'''
# 1. Kh·ªüi t·∫°o
detector = ProductionPipelineFallDetector(model_path, model_name, threshold, config)

# 2. X·ª≠ l√Ω ·∫£nh
result = detector.process_image(pil_image)
# ‚Üì
# 2a. G·ªçi pipeline: inference_result, thumbnail = fall_detector.fall_detect()
# 2b. Convert result: _process_detection_result()
# 2c. Update tracking: _update_tracking()  
# 2d. Enhanced decision: _enhance_decision()

# 3. K·∫øt qu·∫£ enhanced v·ªõi temporal analysis
{
    "fall_detected": True,
    "confidence": 0.85,
    "leaning_angle": 67.5,
    "temporal_confirmation": True,
    "confidence_boosted": True,
    "recent_falls_count": 3,
    "pose_data": {...}
}
'''